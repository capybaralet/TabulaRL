{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: add and explain plots, explain experiments, \n",
    "\n",
    "\n",
    "# Active RL experiment progress report\n",
    "\n",
    "We're looking at Reinforcement learning in Tabular MDPs.\n",
    "In our setting, rewards are only observed when the agent makes a decision to query them (paying some cost to do so).\n",
    "In order to maximize performance (measured as returns minus total query cost), the agent must intelligently chose when to make these queries.\n",
    "\n",
    "So far, we focus on a simple heuristic which queries any state-action which has not already been queried N times, where N is a hyper-parameter.  \n",
    "\n",
    "We propose two ways of tuning this hyperparameter and demonstrate their potential in preliminary experiments. \n",
    "Note that tuning N by running multiple experiments in the environment is assumed not to be an option in our motivating set-up, since this would impose real costs (both in terms of queries, and, potentially, rewards)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment\n",
    "We've been experimenting with gridworld and chain environments. \n",
    "The chain environment is used by Ian Osband in this paper:\n",
    "https://arxiv.org/abs/1607.00215\n",
    "\n",
    "In this environment, the agent must explore a long chain of states, only the last of which has positive expected reward.\n",
    "\n",
    "We'll present results for the chain of length 5.\n",
    "Following Osband, we use PSRL in all of our experiments.\n",
    "\n",
    "Our code is available in a fork of Ian's repo: \n",
    "https://github.com/capybaralet/TabulaRL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here is the implementation of the chain environment:\n",
    "\n",
    "from TabulaRL.environment import TabularMDP\n",
    "def make_stochasticChain(chainLen, max_reward=1):\n",
    "    nState = chainLen\n",
    "    epLen = chainLen\n",
    "    nAction = 2\n",
    "    pNoise = 1. / chainLen\n",
    "\n",
    "    R_true = {}\n",
    "    P_true = {}\n",
    "    for s in xrange(nState):\n",
    "        for a in xrange(nAction):\n",
    "            R_true[s, a] = (0, 0)\n",
    "            P_true[s, a] = np.zeros(nState)\n",
    "\n",
    "    # Rewards (the start and end state rewards are stochastic, all others are determinstically 0.)\n",
    "    R_true[0, 0] = (0, 1)\n",
    "    R_true[nState - 1, 1] = (max_reward, 1)\n",
    "\n",
    "    # Transitions \n",
    "    for s in xrange(nState):\n",
    "        P_true[s, 0][max(0, s-1)] = 1.\n",
    "\n",
    "        # the forward action doesn't always succeed:\n",
    "        P_true[s, 1][min(nState - 1, s + 1)] = 1. - pNoise\n",
    "        P_true[s, 1][max(0, s-1)] += pNoise\n",
    "\n",
    "    stochasticChain = TabularMDP(nState, nAction, epLen)\n",
    "    stochasticChain.R = R_true\n",
    "    stochasticChain.P = P_true\n",
    "    stochasticChain.reset()\n",
    "    return stochasticChain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our experiments\n",
    "\n",
    "Our experiments so far aim at testing the validity of a few of our ideas (and their implementations).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Idea 1: \n",
    "Since the reason PSRL samples environments is to encourage exploration, and we may stop querying (state, action) pairs at some point, we propose using the expected reward (instead of a sample) for any such (s,a).  Our first experiment verifies that this improves performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Idea 2: \n",
    "\n",
    "In order to tune the value of N, we propose evaluating different values of N in environments sampled from the agent's posterior, and chosing the N with the best average performance in these experiments; we call this algorithm \"simulated query rollouts (SQR)\".  \n",
    "\n",
    "We consider an approximate version of SQR (\"ASQR\"). Instead of actually running experiments in the simulated environments (which can be prohibitively costly), ASQR instead:\n",
    "1. Samples an environment, E, from the agents posterior\n",
    "2. Performs all N queries of each (s,a) off-line without acting in E\n",
    "3. Updates the agent's posterior based on the results of these queries\n",
    "4. Finds the expected environment, E', based on this updated posterior\n",
    "5. Uses planning to find the optimal policy for E', and computes the expected return of running this policy in E\n",
    "6. Selects the N with the best performance (= expected return - cost of queries).\n",
    "\n",
    "We evaluate both of these algorithms as well, and show that their estimates of the best N do not deviate drastically from the best N found by hyper-parameter search.  \n",
    "\n",
    "\n",
    "For a more thorough description of SQR and ASQR, see: https://github.com/capybaralet/TabulaRL/blob/master/tex/Choosing%20Which%20Queries%20to%20Make.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# TODO: more comments in the code\n",
    "Here, we include a modified version of the script we use to run these experiments (dk_exp_script.py), removing parts which don't work in an ipython notebook (like argparser). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ArgumentError",
     "evalue": "argument --log_n_max: conflicting option string(s): --log_n_max",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mArgumentError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-971987b83f2a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# SETUP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# N.B. These need to be set!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_argument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'--log_n_max'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_argument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'--log_num_episodes'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_argument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'--num_R_samples'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/david/anaconda/lib/python2.7/argparse.pyc\u001b[0m in \u001b[0;36madd_argument\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1306\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"length of metavar tuple does not match nargs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1308\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1310\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0madd_argument_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/david/anaconda/lib/python2.7/argparse.pyc\u001b[0m in \u001b[0;36m_add_action\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m   1680\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_add_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1681\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption_strings\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1682\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optionals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1683\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1684\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_positionals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/david/anaconda/lib/python2.7/argparse.pyc\u001b[0m in \u001b[0;36m_add_action\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m   1507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1508\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_add_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1509\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ArgumentGroup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1510\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_group_actions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1511\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/david/anaconda/lib/python2.7/argparse.pyc\u001b[0m in \u001b[0;36m_add_action\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m   1320\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_add_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0;31m# resolve any conflicts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_conflict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m         \u001b[0;31m# add to actions list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/david/anaconda/lib/python2.7/argparse.pyc\u001b[0m in \u001b[0;36m_check_conflict\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m   1458\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconfl_optionals\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1459\u001b[0m             \u001b[0mconflict_handler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1460\u001b[0;31m             \u001b[0mconflict_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfl_optionals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1462\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_handle_conflict_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconflicting_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/david/anaconda/lib/python2.7/argparse.pyc\u001b[0m in \u001b[0;36m_handle_conflict_error\u001b[0;34m(self, action, conflicting_actions)\u001b[0m\n\u001b[1;32m   1465\u001b[0m                                      \u001b[0;32mfor\u001b[0m \u001b[0moption_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1466\u001b[0m                                      in conflicting_actions])\n\u001b[0;32m-> 1467\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mArgumentError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mconflict_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1468\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1469\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_handle_conflict_resolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconflicting_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mArgumentError\u001b[0m: argument --log_n_max: conflicting option string(s): --log_n_max"
     ]
    }
   ],
   "source": [
    "# %load dk_exp_script.py\n",
    "import numpy as np\n",
    "import argparse\n",
    "from collections import defaultdict\n",
    "\n",
    "import TabulaRL.gridworld as gridworld\n",
    "import TabulaRL.query_functions as query_functions\n",
    "import TabulaRL.finite_tabular_agents as finite_tabular_agents\n",
    "from TabulaRL.feature_extractor import FeatureTrueState\n",
    "from TabulaRL.environment import make_stochasticChain\n",
    "\n",
    "\n",
    "#-----------------------------------------------------------------------------------\n",
    "# USEFUL FUNCTIONS\n",
    "\n",
    "def is_power2(num):\n",
    "    'states if a number is a power of two'\n",
    "    return num != 0 and ((num & (num - 1)) == 0)\n",
    "\n",
    "def sample_gaussian(loc, scale, shape):\n",
    "    if scale == 0:\n",
    "        return loc * np.ones(shape)\n",
    "    else:\n",
    "        return np.random.normal(loc, scale, shape)\n",
    "\n",
    "#-----------------------------------------------------------------------------------\n",
    "# SETUP\n",
    "algorithm='fixed_n'  \n",
    "algorithm='SQR'  \n",
    "algorithm='ASQR'  \n",
    "\n",
    "log_n_max=10\n",
    "log_num_episodes=10\n",
    "num_R_samples=20\n",
    "query_cost = 1.\n",
    "normalize_rewards=0\n",
    "\n",
    "save_str = 'toy_experiment/'\n",
    "\n",
    "# ENVIRONMENT\n",
    "chain_len = 5\n",
    "epLen = chain_len\n",
    "env = make_stochasticChain(chain_len, max_reward=((chain_len - 1.)/chain_len)**-chain_len)\n",
    "f_ext = FeatureTrueState(env.epLen, env.nState, env.nAction, env.nState)\n",
    "\n",
    "# AGENT\n",
    "alg = finite_tabular_agents.PSRLLimitedQuery\n",
    "\n",
    "# RUN EXPERIMENT\n",
    "initial_env = env\n",
    "num_episodes_remaining = 2**log_num_episodes\n",
    "n_max = 2**log_n_max\n",
    "ns = np.hstack((np.array([0,]), 2**np.arange(log_n_max)))\n",
    "\n",
    "# save results here:\n",
    "num_queries = np.empty((num_R_samples, log_num_episodes+1, log_n_max+1))\n",
    "returns = np.empty((num_R_samples, log_num_episodes+1, log_n_max+1))\n",
    "returns_max_min = np.empty((num_R_samples, 2))\n",
    "\n",
    "\n",
    "for kk in range(num_R_samples):\n",
    "    print \"beginning experiment #\", kk\n",
    "    env = copy.deepcopy(initial_env)\n",
    "\n",
    "    if algorithm in ['SQR', 'ASQR']: # use a sampled environment\n",
    "        sampled_R, sampled_P = initial_agent.sample_mdp()\n",
    "        env.R = {kk:(sampled_R[kk], 1) for kk in sampled_R}\n",
    "        env.P = sampled_P\n",
    "        returns_max_min[kk,0] = initial_agent.compute_qVals(sampled_R, sampled_P)[1][0][0]\n",
    "        returns_max_min[kk,1] = - initial_agent.compute_qVals({kk: -sampled_R[kk] for kk in sampled_R}, sampled_P)[1][0][0]\n",
    "\n",
    "    sampled_rewards = {(s,a) : sample_gaussian(env.R[s,a][0], env.R[s,a][1], n_max) for (s,a) in env.R.keys()}\n",
    "    # is this still needed??\n",
    "    first_n_sampled_rewards = [{sa: sampled_rewards[sa][:n] for sa in sampled_rewards} for n in range(n_max + 1)]\n",
    "    for ind, n in enumerate(ns):\n",
    "        agent = copy.deepcopy(initial_agent)\n",
    "        if environment.startswith('grid'): # FIXME: update agent\n",
    "            query_function = QueryFixedFunction(query_cost, lambda s,a: (a==0) * n)\n",
    "        else:\n",
    "            query_function = query_functions.QueryFirstNVisits(query_cost, n)\n",
    "        query_function.setAgent(agent)\n",
    "\n",
    "        if algorithm==\"ASQR\": # update posterior, compute expected returns\n",
    "            updated_R = {}\n",
    "            for [s,a] in first_n_sampled_rewards[n]:\n",
    "                mu0, tau0 = agent.R_prior[s,a]\n",
    "                num_samples = len(first_n_sampled_rewards[n][s,a])\n",
    "                tau1 = tau0 + agent.tau * num_samples\n",
    "                mu1 = (mu0 * tau0 + sum(first_n_sampled_rewards[n][s,a]) * agent.tau) / tau1\n",
    "                updated_R[s,a] = mu1\n",
    "            updated_P = sampled_P\n",
    "            expected_returns = agent.compute_qVals_true(updated_R, updated_P, sampled_R, sampled_P)[0]\n",
    "            returns[kk, :, ind] = expected_returns * 2**np.arange(log_num_episodes+1)\n",
    "            num_queries[kk, :, ind] = n * sum([agent.query_function.will_query(s,a) for [s,a] in first_n_sampled_rewards[n]])\n",
    "        else: # Run an experiment \n",
    "            nEps = num_episodes_remaining\n",
    "            # --------------- modified from dk_run_finite_tabular_experiment ------------------\n",
    "            qVals, qMax = env.compute_qVals()\n",
    "            cumReward = 0\n",
    "            cumQueryCost = 0 \n",
    "            for ep in xrange(1, nEps + 2):\n",
    "                env.reset()\n",
    "                epMaxVal = qMax[env.timestep][env.state]\n",
    "                agent.update_policy(ep)\n",
    "\n",
    "                pContinue = 1\n",
    "                while pContinue > 0:\n",
    "                    # Step through the episode\n",
    "                    h, oldState = f_ext.get_feat(env)\n",
    "\n",
    "                    action = agent.pick_action(oldState, h)\n",
    "                    query, queryCost = agent.query_function(oldState, action, ep, h)\n",
    "                    cumQueryCost += queryCost\n",
    "\n",
    "                    reward, newState, pContinue = env.advance(action)\n",
    "                    if query and first_n_sampled_rewards[n] is not None:\n",
    "                        reward = first_n_sampled_rewards[n][oldState, action][agent.query_function.visit_count[oldState, action] - 1]\n",
    "                    cumReward += reward \n",
    "                    agent.update_obs(oldState, action, reward, newState, pContinue, h, query)\n",
    "\n",
    "                if is_power2(ep): # checkpoint\n",
    "                    returns[kk, int(np.log2(ep)), ind] = cumReward\n",
    "                    num_queries[kk, int(np.log2(ep)), ind] = cumQueryCost / query_cost\n",
    "\n",
    "            # ---------------------------------------------------------------------\n",
    "    np.save(save_str + 'num_queries', num_queries)\n",
    "    np.save(save_str + 'returns', returns)\n",
    "    np.save(save_str + 'returns_max_min', returns_max_min)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis (TODO: rm?)\n",
    "The way we've implemented the above experiments, we can use our saved results to compute the performance for many different query costs and numbers of episodes.\n",
    "\n",
    "So, for instance, if we want to see how performance changes as a function of "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
