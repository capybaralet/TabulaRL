\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{natbib}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{mathtools}

\usepackage{algorithm}
\usepackage[noend]{algpseudocode}

\title{Choosing Which Queries to Make}
\author{David Krueger, John Salvatier}
\date{August 2016} 
\begin{document}
\maketitle




We're proposing a few methods for how to choose the number of queries per state-action in model-based active RL.
We consider the tabular MDP case, with state and actions sets $\mathcal{S}, \mathcal{A}$, and transition and reward functions $\mathcal{T}, \mathcal{R}$.
We use $r$ to denote instantaneous rewards.

We suppose each query has a fixed cost of $c$, and let $\mathcal{Q} := \mathbb{N}^{| \mathcal{S} \times \mathcal{A} |}$ represent the set of all possible sets of queries.
Then let $q_t \in \mathcal{Q}$ be the set of queries which have been performed at time $t$.
We can make our decision to query (or not) by specifying some \emph{desired query set}, $q^{max}_t \in \mathcal{Q}$, representing to set of all queries we'd like to make, and querying whenever we have not yet made all of those queries, i.e. if $q_t < q^{max}_t$.

We measure performance as returns minus total query cost, i.e. $c$ times the number of queries performed.
Our baseline heuristic considers a fixed desired query set, $q^{max}_t = (n,...,n)$, with $n$ as a hyperparameter.
It seems clear that the optimal $n \rightarrow \infty$ as $c \rightarrow 0$, or $\gamma \rightarrow 1$, and $n \rightarrow 0$ as $c \rightarrow \infty$ or $\gamma \rightarrow 0$.
But it's not clear what specific value of $n$ to choose.
Since we care about our cost during learning, we'd like a way to choose $n$ before learning (or adjusting it during learning).

We use the $\mathop{PSRL}$ algorithm with a slight modification: we fix the agent's prior over rewards for a given state and action to be its expectation when no more queries of that state-action pair will be performed.
% NTS: when we have an idea of how many queries remain, we can use that to both:
%	account for the cost of the queries somehow
%	smooth the distribution towards a delta (i.e. artificially decrease variance)
We use $\mathop{PSRL}_n$ to denote this modification of $\mathop{PSRL}$ using with $q^{max}_t = (n,...,n)$.

% Simulated Query Rollouts
\section{A Proposal for Tuning $n$}
We propose tuning $n$ via Simulated Query Rollouts (SQR). 

The basic idea is to sample $k$ different reward functions (or, more generally, environments) $\mathcal{\tilde{R}}_1, ..., \mathcal{\tilde{R}}_k$ from the agent's prior on $\mathcal{R}$, and evaluate the agent's performance on each of these simulated worlds for different query strategies, e.g. different values of $n$.
We then use the best performing query strategy to act in the real environment.
This strategy may be sensitive to differences between the prior and the real environment.



\begin{algorithm}
\caption{Simulated Query Rollouts}
\label{SQR}
\begin{algorithmic}[1]
\For{$i \in \{1,...,k\}$}
	\State Sample $\tilde{\mathcal{R}_i} \sim P(\mathcal{R})$
	\State Sample $\{\tilde{r}\}_i \sim \tilde{\mathcal{R}_i}$
	\For {$n \in \{0, ..., N\}$}
		\State Run $\mathop{PSRL}_n$, using pre-sampled rewards $\{\tilde{r}\}_i$.
		\State Record the resulting performance: $\mathit{perf}_i(\mathop{PSRL}_n)$
		% \item compute actual performance from the discounted returns minus discounted query costs. $ P_{n,i} = \sum_{t=0} \gamma^i (r_t - \mathbb{I}[q_{t,s,a} < n]c)  $.
	\EndFor
\EndFor
\Return $n = \mathop{\mathrm{argmax}} \frac{1}{k} \sum_{i=1}^k \mathit{perf}_i(\mathop{PSRL}_n)$ 
%\EndProcedure
\end{algorithmic}
\end{algorithm}

Note that:

$
\lim_{k \rightarrow \infty} \frac{1}{k} \sum_{i=1}^k \mathit{perf}_i(\mathop{PSRL}_n) = \mathbb{E} \: \mathit{perf}(\mathop{PSRL}_n).
$

Also, note that we propose using the same instantaneous sampled rewards $\{\tilde{r}\}_i$ for each value of $n$ in each environment $\tilde{\mathcal{R}_i}$, in order to reduce stochasticity.

The running time to compute $n$ here is roughly $N * k * \mathop{PSRL}(S, A, T) $

%As $k$ gets large, the expectation of the performance should be accurate and $n$ will be optimal. Of course the computation is relatively inefficient. 
% ^ is this true?  Doesn't it depend on how well our prior matches the ground truth??

% Approximate Simulated Query Rollouts
\section{A Cheaper Approximation}
We now present a procedure for Approximate Simulated Query Rollout (ASQR).
To remove the need to run $\mathop{PSRL}$ in line 5 of SQR (Algorithm~\ref{SQR}), we can instead:

\begin{enumerate}
%\item Simulate all of the queries in the desired query set via sampling the necessary $\{\tilde{r}\} \sim \tilde{\mathcal{R}}$.
\item Update the agent's posterior over reward functions using all the queries in its desired query set.% $P_{agent} (\mathcal{R} | \{\tilde{r}\}_i)$ .
\item Do planning and compute expected performance under this updated posterior (subtracting the cost of all the simulated queries).
\end{enumerate}

This only involves one iteration of planning, unlike $\mathop{PSRL}$, which performs planning numerous times, e.g. once per episode.
This approach may under-estimate performance because the actual $\mathop{PSRL}$ agent might not end up making all of the desired queries (and hence pay less query cost).
Or it may over-estimate performance, since the $\mathop{PSRL}$ agent does not have the benefit of observing the queries up-front, and must make decisions with less information during learning.
These differences would be especially large when many states or important states are not very reachable (and so would be queried more slowly or less often by the $\mathop{PSRL}$ agent).

% ASQR in the loop
\section{Further Extensions}
We can run the above algorithms at any time during learning, in order to dynamically adjust our desired query set based on new information we've acquired.
At a high level, this is analogous to the way Thompson Sampling resamples throughout learning.

As a simple heuristic, we propose running ASQR with $k=1$ every $N$-th time an environment is sampled in Thompson Sampling.
In the worst case, this algorithm doubles the amount of times that planning must be performed.
It would probably be better to invest more computation up-front in choosing good desired query sets during the early stages of learning.


Since querying each state equally often is likely highly suboptimal, we'd like to consider a broader class of desired query sets, but the number of such sets grows exponentially in the total number of queries.
We believe techniques for linear bandits might be used to specify the priority of each possible query at a given moment, thus specifying a strict total order over desired query sets (with the smallest element being the queries already performed).
Then we could use the above techniques to instead tune which index in this order to select as $q^{max}_t$ at any given $t$.

The connection with linear bandits comes from viewing each state, action pair, $(s,a)$, as one dimension of arm-space.
Note that given transition operator $\mathcal{T}$, each policy, $\pi$ induces a distribution over $(s,a)$, and hence a point on the $L_1$ sphere in arm-space.
Unlike a standard linear bandit, however, our queries are restricted to standard basis vectors, while only the arms corresponding to valid policies can ultimately be exploited.
Furthermore, an agent must visit a state, action in order to query it (although notably, this is \emph{not} the case for ASQR.




\section{Importance Sampling for Off-Policy Querying}
Our query decisions might depend on several things, besides the state and action:
\begin{enumerate}
\item The agent's beliefs
\item The visit/query counts (or more generally, the whole history)
\end{enumerate}

We can think of (1) as providing a compressed version of (2).  
To frame the query function as part of the policy, we need to augment the state space with some extra information of this form.

Using this framing allows us to learn about a query functions based on the experience of an agent with a different query function (if the functions are stochastic).
This is analagous to the "branching agent" idea that Owain mentioned.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bibliographystyle{plainnat}
%\bibliographystyle{plain}
\bibliography{references}
\end{document}
