\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{natbib}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{mathtools}

\usepackage{algorithm}
\usepackage[noend]{algpseudocode}

\title{Scraps and Notes}
\author{David Krueger, John Salvatier}
\date{August 2016} 
\begin{document}
\maketitle


\section{Learning $T$ first}
Here, we'd like to give an example MDP where the correct strategy is to avoid querying until $T$ is known.
Let $\mathcal{S} = \{s_0,s_1,s_L, s_R\}$, $\mathcal{A} = \{a_L, a_R\}$, and $T(s_0) = s_1, T(s_1, a_L) = s_L, T(s_1, a_R) = a_R, T(s_L) = T(s_R) = s_0$.
And let $R(s_0) = 0, R(s_1) = 100, R(s_L) = 1, R(s_R) = 10$.
Once the agent knows $T$, it can recognize this as (essentially) a bandit problem, and determine that choosing $a_R$ for state $s_1$ is optimal.
% TODO...




\section{Function Approximation}
When we use function approximation, we can maybe replace linear bandit methods with something like the Active Reward Learning method (i.e. a model of the rewards, GP or DNN)

\section{Importance Sampling for Off-Policy Querying}
Our query decisions might depend on several things, besides the state and action:
\begin{enumerate}
\item The agent's beliefs
\item The visit/query counts (or more generally, the whole history)
\end{enumerate}

We can think of (1) as providing a compressed version of (2).  
To frame the query function as part of the policy, we need to augment the state space with some extra information of this form.

Using this framing allows us to learn about a query functions based on the experience of an agent with a different query function (if the functions are stochastic).
This is analagous to the "branching agent" idea that Owain mentioned.






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bibliographystyle{plainnat}
%\bibliographystyle{plain}
\bibliography{references}
\end{document}
