\documentclass[a4paper]{article}

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{enumerate}
\usepackage{hyperref}

\usepackage{natbib}
\bibliographystyle{plainnat}
\bibpunct{(}{)}{;}{a}{,}{,}

\usepackage{mythm}

\DeclareMathOperator*{\argmax}{arg\,max} % argmax operator


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{Active Multi-Armed Bandits}
\author{Jan Leike et al.}
\date{\today}

\maketitle


\begin{abstract}%
We introduce the \emph{active multi-armed bandit problem},
a cross-over between multi-armed bandits and active learning.
% Despite being a slight variation on the simplest reinforcement learning problem,
% active multi-armed bandits turn out to be quite challenging:
As a sub-problem of the hard class of partial monitoring problems,
the problem incurs $\Theta(n^{2/3})$ worst-case regret.
Moreover,
the quality of an arm is not independent of the other arms
and greedy information-gathering strategies perform poorly.
% Therefore good strategies need to think globally about the problem.
We compare partial monitoring algorithms on the problem and
introduce a new algorithm that outperforms them.
However,
the central question to solving active multi-armed bandits,
how to efficiently quantify the long-term value of information,
remains open.
\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Problem Formulation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The \emph{active multi-armed bandit problem} is
an online learning problem
where the learner has to select from $K$ actions (`arms') in every round.
Choosing action $I_t$ in round $t$ incurs a payoff
$X_t$ of drawn from a time-independent distribution $\nu_{I_t}$ with mean $\mu_i$
unknown to the learner.
In contrast to the classical multi-armed bandit problem,
the learner does not observe the payoff
unless they pay a fixed \emph{query cost $c > 0$}.
This cost is the same for every arm and every round.
% TODO: cross-over between multi-armed bandits and active learning

Let $\mu^* := \max_i \mu_i$ denote the mean of the best arm.
The goal of the learner is to maximize expected payoff up to a horizon $n$,
or, equivalently, to minimize the \emph{expected regret}
\[
R_n = \mathbb{E} \left[ \sum_{t=1}^n (\mu^* - X_t - c Q_t) \right],
\]
where $Q_t$ is $1$ if the learner chooses to observe the reward in round $t$ and $0$ otherwise.
In other words, the expected regret is the expected payoff
that is lost
because the learner did not blindly choose the best option in every round.
% (note that $Q_t$ is a random variable).

\paragraph{Notation.}
Let $i^* := \argmax_i \mu_i$ be an index of an optimal arm,
let $\Delta_i := \mu^* - \mu_i$, and
let $\Delta_{\min} := \min_{i \neq i^*} \Delta_i$.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Properties}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The active multi-armed bandit problem is
a subproblem of the partial monitoring problem~\citep{Piccolboni01}.
People usually consider the adversarial setting,
which incurs $0$, $\mathcal{O}(\sqrt{n})$, $\mathcal{O}(n^{2/3})$, or linear regret~\citep{Antos13}.
Note: partial monitoring in finite! (e.g. Bernoulli bandits)
Our setting is stochastic~\citep{Komiyama15}.
% TODO: partial monitoring with graph feedback, see "Online learning with feedback graphs: beyond bandits." COLT 2015.

TODO: in active bandits, distinguishing two close arms incurs regret linear in the number of time steps needed to distinguish them, whereas in (regular) bandits the regret grows proportionally to the gap size, which is $n^{1/2}$ in the worst case.
In other words, in (regular) bandit problems it is much cheaper
to distinguish very close arms.

\begin{proposition}[Cost-Independent Regret Bound]
The worse-case expected regret is $R_n \in \Theta(n^{2/3})$.
% and the problem-dependent regret bound is $R_n \in \Theta(\sum_{i: \Delta_i > 0} \log(n)/\Delta_i)$.
\end{proposition}
\begin{proof}[Proof sketch.]
Consider an active two-armed bandit problem
where the gap between the arms is $\Delta := n^{-1/3}$.
Commiting blindly to one of the arms incurs a regret of
$0.5n \cdot n^{-1/3} \in \Theta(n^{2/3})$.
Alternatively, to distinguish the arms,
we need about $\Theta(1/\Delta^2)$ steps.
In every step we need to pay the cost $c$ to see the payoff,
which incurs a regret of
at least $c\Theta(1/\Delta^2) = \Theta(cn^{2/3})$.
% This also leads to the problem-dependent regret bound of $\Theta(1/\Delta_{\min}^2)$.
\end{proof}

Notably, to achieve optimal asymptotic regret, you can ignore the query cost since it is constant.
We are particularly interested in optimizing
the cost-dependent regret.
Partial monitoring algorithms are designed for optimal asymptotic regret rate and hence do not take the magnitude of the query cost into account.

TODO: what is the cost-dependent regret rate?

It is well-known that to solve partial monitoring problems
it is sometimes necessary to take actions that you strongly believe to be suboptimal to get information.
For active bandits, paying to see the reward is always suboptimal
by at least $c > 0$.
Nevertheless, if we never pay the cost, we learn nothing about the problem.
Because of this, stategies like optimism or Thompson sampling cannot work for this problem.

Moreover, there can be no index strategy~\citep[Ex.~4]{Hay12}.
An index strategy computes a number for each action
and takes the action with highest number.
This requires that
the the quality of an action is independent of the alternatives.
Consider the following 3-armed active bandit problem
where the decision between arm 1 and arm 2 depends
on the value of arm 3:
The arms' payoffs are deterministic but unknown with
either $-1.5$ or $1.5$ for arm 1 with equal probability,
either $0.25$ or $1.75$ for arm 2 with equal probability, and
$\lambda$ for arm 3.
The horizon is $n = 1000$
and the query cost is $c = 200$.
% for $\lambda = 1.31$ observing arm 1 is not worth the investment, but observing arm 2 is worth $n (1.75 - \lambda) \cdot 0.5 - 200 = 40 > 0$.
% For $\lambda = -10$, observing arm 1 and then blindly switching to arm 2 if the payoff is $-1.5$ yields expected value of $((n-1)1 - 1.5) \cdot 0.5 + n 1.5 \cdot 0.5) - 200 = 1047.5$.
% Blindly commiting to the second arm has an expected value of $1000$. Observing the second arm and then observing the first arm if the second arm is $0.25$ has an expected value of
% $(n1.75 - 200) \cdot 0.5 + (0.25 + (n-1)1.5 - 400) \cdot 0.25 + (-1.5 + (n-1)0.25 - 400) \cdot 0.25 = 1011.75$.
% Hence observing arm 2 is optimal for $\lambda = 1.31$ and observing arm 1 is optimal for $\lambda = -10$.
% In short:
If $\lambda$ is high (e.g.\ $1.31$),
we want to observe the arm with the high maximum first
since the other arm is known to be suboptimal, and
if $\lambda$ is low (e.g.\ $-10$),
we want to observe the arm with high variance first
because in case that arm is bad, we are sure that
the high mean arm is better and don't need to observe it.

%then observing mu_1 is -0.2 in expectation, but in case the observation is -1.5, it's worth switching to arm 2 and we get an expectation of 1 - 0.2 = 0.8. If the payoff is 1.5, then it's not worth observing/switching to arm 2. So we get 0.8 * 0.5 + 1.3 * 0.5 = 1.05. Blindly commiting to arm 2 is 1. Observing mu_2 = 0.25 leads us to query mu_1 to get 1.5*0.5 + 0.25*0.5 - 2*0.2 = 0.475. Hence pull second first has 1.55*0.5 + 0.475*0.5 = 1.0125.
% Need n = 1000 and rescale rewards by 0.001 so that the query steps don't matter.
% In short: if lambda is high, observe the high max arm, and if lambda is low, observe the high variance arm first (because if it's bad, you don't need to observe the other one, just commit to it).


If we query in every time step, then regret is linear.
The optimal strategy is to query for the first $q$ time steps and then stop querying.
If you don't query, then in the next step you face the same information, except that the horizon is now shorter,
so the value of information can only have diminished.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Algorithms}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The Bayes-optimal solution to an active multi-armed Bernoulli bandit problem can be found with dynamic programming.
The corresponding belief MDP has $\mathcal{O}(n^{2K})$ states
and can be solved in $\mathcal{O}(n^{2K})$ time steps.
This is polynomial for constant $K$,
but completely prohibitive in practice.

DMED seems to achieve the optimal cost-independent regret rate.

We are looking for a (heuristic) algorithm that achieves optimal asymptotic regret rate, performs well in practice and has constant or linear time complexity.

A natural choice would be to take an explore-then-exploit algorithm, such as for \emph{budgeted bandits}~\citep{...}.
The crucial question is how to decide to stop querying.
In other words, what is the (long-term) value of information of an additional query?

This problem does not have a greedy solution.
If one additional data point does not change my opinion
which arm I currently consider to be best,
then a greedy strategy considers the value of information to be zero and stop querying prematurely.
Therefore we have to take into account the long-range effects of querying for multiple time steps.
% TODO: tricky because the posterior is nonstationary.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experiments}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\iffalse
Algorithms:
* vanilla DMED (with commitment when 1 arm is selected?)
* partial monitoring algorithm (?)
* optimal stopping time (optimized problem-dependent)
* heuristic (with optimized alpha)

Problems:
* medium query cost c = 2 (>> gap)
  * horizon n = 10k vs gap 0.1 (< n^2/3)
  * horizon n = 1k vs gap 0.1 (~ n^2/3)
  * horizon n = 10k vs gap 0.01 (don't identify the arms)
* high query cost c = 50
  * horizon n = 10k vs gap 0.3 (pay to identify)
  * horizon n = 10k vs gap 0.1 (borderline case)
  * horizon n = 10k vs gap 0.05 (do not pay to identify)
* low query cost c = 0.01 (c << gap)
  * horizon n = 10k vs gap 0.1 (should almost always find the best arm)
  * horizon n = 200 vs gap 0.1 (query all the way?)
\fi


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Outlook}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Open questions:
\begin{itemize}
\item Is there a good parameter-free algorithm?
\end{itemize}


\paragraph{Acknowledgements.}
Tor Lattimore, Reimar Leike, Ryan Lowe, Jessica Taylor, \dots
% obviously: Owain + John + David (co-authors?)

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibliography{references}


\end{document}
